{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Reshape, Flatten, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU,TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers, regularizers, optimizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import csv\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachin\\Desktop\\fake news\\LIAR PLUS dataset\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\sachin\\Desktop\\fake news\\LIAR PLUS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LIAR PLUS dataset\n",
    "train=pd.read_csv('train2.tsv',delimiter='\\t',encoding='utf-8')\n",
    "test=pd.read_csv('test2.tsv',delimiter='\\t',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = [\"index\",\"ID\" ,\"label\",\"statement\",\"subject\",\"speaker\",\"speaker's job title\",\"state info\",\"party affiliation\",\"barely true counts\",\"false counts\",\"half true counts\",\"mostly true counts\",\"pants on fire counts\",\"the context\",\"the extracted justification\"]\n",
    "train.columns = [\"index\",\"ID\" ,\"label\",\"statement\",\"subject\",\"speaker\",\"speaker's job title\",\"state info\",\"party affiliation\",\"barely true counts\",\"false counts\",\"half true counts\",\"mostly true counts\",\"pants on fire counts\",\"the context\",\"the extracted justification\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker's job title</th>\n",
       "      <th>state info</th>\n",
       "      <th>party affiliation</th>\n",
       "      <th>barely true counts</th>\n",
       "      <th>false counts</th>\n",
       "      <th>half true counts</th>\n",
       "      <th>mostly true counts</th>\n",
       "      <th>pants on fire counts</th>\n",
       "      <th>the context</th>\n",
       "      <th>the extracted justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "      <td>Surovell said the decline of coal \"started whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "      <td>Obama said he would have voted against the ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "      <td>The release may have a point that Mikulskis co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "      <td>Crist said that the economic \"turnaround start...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>12465.json</td>\n",
       "      <td>true</td>\n",
       "      <td>The Chicago Bears have had more starting quart...</td>\n",
       "      <td>education</td>\n",
       "      <td>robin-vos</td>\n",
       "      <td>Wisconsin Assembly speaker</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a an online opinion-piece</td>\n",
       "      <td>But Vos specifically used the word \"fired,\" wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10234</td>\n",
       "      <td>10235</td>\n",
       "      <td>5473.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>There are a larger number of shark attacks in ...</td>\n",
       "      <td>animals,elections</td>\n",
       "      <td>aclu-florida</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>interview on \"The Colbert Report\"</td>\n",
       "      <td>They compounded their error by combining full ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10235</td>\n",
       "      <td>10236</td>\n",
       "      <td>3408.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Democrats have now become the party of the [At...</td>\n",
       "      <td>elections</td>\n",
       "      <td>alan-powell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>an interview</td>\n",
       "      <td>Romney said that \"Obamacare  means that for up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10236</td>\n",
       "      <td>10237</td>\n",
       "      <td>3959.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>Says an alternative to Social Security that op...</td>\n",
       "      <td>retirement,social-security</td>\n",
       "      <td>herman-cain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>republican</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>a Republican presidential debate</td>\n",
       "      <td>But that it leaves out important details and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10237</td>\n",
       "      <td>10238</td>\n",
       "      <td>2253.json</td>\n",
       "      <td>false</td>\n",
       "      <td>On lifting the U.S. Cuban embargo and allowing...</td>\n",
       "      <td>florida,foreign-policy</td>\n",
       "      <td>jeff-greene</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a televised debate on Miami's WPLG-10 against ...</td>\n",
       "      <td>We checked the research and, quite frankly, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10238</td>\n",
       "      <td>10239</td>\n",
       "      <td>1155.json</td>\n",
       "      <td>pants-fire</td>\n",
       "      <td>The Department of Veterans Affairs has a manua...</td>\n",
       "      <td>health-care,veterans</td>\n",
       "      <td>michael-steele</td>\n",
       "      <td>chairman of the Republican National Committee</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>a Fox News interview</td>\n",
       "      <td>Krueger said that \"since the Affordable Care A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10239 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index          ID        label  \\\n",
       "0          1  10540.json    half-true   \n",
       "1          2    324.json  mostly-true   \n",
       "2          3   1123.json        false   \n",
       "3          4   9028.json    half-true   \n",
       "4          5  12465.json         true   \n",
       "...      ...         ...          ...   \n",
       "10234  10235   5473.json  mostly-true   \n",
       "10235  10236   3408.json  mostly-true   \n",
       "10236  10237   3959.json    half-true   \n",
       "10237  10238   2253.json        false   \n",
       "10238  10239   1155.json   pants-fire   \n",
       "\n",
       "                                               statement  \\\n",
       "0      When did the decline of coal start? It started...   \n",
       "1      Hillary Clinton agrees with John McCain \"by vo...   \n",
       "2      Health care reform legislation is likely to ma...   \n",
       "3      The economic turnaround started at the end of ...   \n",
       "4      The Chicago Bears have had more starting quart...   \n",
       "...                                                  ...   \n",
       "10234  There are a larger number of shark attacks in ...   \n",
       "10235  Democrats have now become the party of the [At...   \n",
       "10236  Says an alternative to Social Security that op...   \n",
       "10237  On lifting the U.S. Cuban embargo and allowing...   \n",
       "10238  The Department of Veterans Affairs has a manua...   \n",
       "\n",
       "                                  subject         speaker  \\\n",
       "0      energy,history,job-accomplishments  scott-surovell   \n",
       "1                          foreign-policy    barack-obama   \n",
       "2                             health-care    blog-posting   \n",
       "3                            economy,jobs   charlie-crist   \n",
       "4                               education       robin-vos   \n",
       "...                                   ...             ...   \n",
       "10234                   animals,elections    aclu-florida   \n",
       "10235                           elections     alan-powell   \n",
       "10236          retirement,social-security     herman-cain   \n",
       "10237              florida,foreign-policy     jeff-greene   \n",
       "10238                health-care,veterans  michael-steele   \n",
       "\n",
       "                                 speaker's job title state info  \\\n",
       "0                                     State delegate   Virginia   \n",
       "1                                          President   Illinois   \n",
       "2                                                NaN        NaN   \n",
       "3                                                NaN    Florida   \n",
       "4                         Wisconsin Assembly speaker  Wisconsin   \n",
       "...                                              ...        ...   \n",
       "10234                                            NaN    Florida   \n",
       "10235                                            NaN    Georgia   \n",
       "10236                                            NaN    Georgia   \n",
       "10237                                            NaN    Florida   \n",
       "10238  chairman of the Republican National Committee   Maryland   \n",
       "\n",
       "      party affiliation  barely true counts  false counts  half true counts  \\\n",
       "0              democrat                 0.0           0.0               1.0   \n",
       "1              democrat                70.0          71.0             160.0   \n",
       "2                  none                 7.0          19.0               3.0   \n",
       "3              democrat                15.0           9.0              20.0   \n",
       "4            republican                 0.0           3.0               2.0   \n",
       "...                 ...                 ...           ...               ...   \n",
       "10234              none                 0.0           1.0               1.0   \n",
       "10235        republican                 0.0           0.0               0.0   \n",
       "10236        republican                 4.0          11.0               5.0   \n",
       "10237          democrat                 3.0           1.0               3.0   \n",
       "10238        republican                 0.0           1.0               1.0   \n",
       "\n",
       "       mostly true counts  pants on fire counts  \\\n",
       "0                     1.0                   0.0   \n",
       "1                   163.0                   9.0   \n",
       "2                     5.0                  44.0   \n",
       "3                    19.0                   2.0   \n",
       "4                     5.0                   1.0   \n",
       "...                   ...                   ...   \n",
       "10234                 1.0                   0.0   \n",
       "10235                 1.0                   0.0   \n",
       "10236                 3.0                   3.0   \n",
       "10237                 0.0                   0.0   \n",
       "10238                 0.0                   2.0   \n",
       "\n",
       "                                             the context  \\\n",
       "0                                        a floor speech.   \n",
       "1                                                 Denver   \n",
       "2                                         a news release   \n",
       "3                                    an interview on CNN   \n",
       "4                              a an online opinion-piece   \n",
       "...                                                  ...   \n",
       "10234                  interview on \"The Colbert Report\"   \n",
       "10235                                       an interview   \n",
       "10236                   a Republican presidential debate   \n",
       "10237  a televised debate on Miami's WPLG-10 against ...   \n",
       "10238                               a Fox News interview   \n",
       "\n",
       "                             the extracted justification  \n",
       "0      Surovell said the decline of coal \"started whe...  \n",
       "1      Obama said he would have voted against the ame...  \n",
       "2      The release may have a point that Mikulskis co...  \n",
       "3      Crist said that the economic \"turnaround start...  \n",
       "4      But Vos specifically used the word \"fired,\" wh...  \n",
       "...                                                  ...  \n",
       "10234  They compounded their error by combining full ...  \n",
       "10235  Romney said that \"Obamacare  means that for up...  \n",
       "10236  But that it leaves out important details and t...  \n",
       "10237  We checked the research and, quite frankly, fi...  \n",
       "10238  Krueger said that \"since the Affordable Care A...  \n",
       "\n",
       "[10239 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10239, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop nan values\n",
    "count=0\n",
    "for index, row in train.iterrows():\n",
    "#     print(1)\n",
    "    if type(train[\"the extracted justification\"][index])!=str or type(train[\"statement\"][index])!=str:\n",
    "        count+=1\n",
    "        train.drop(index, inplace=True)\n",
    "        continue\n",
    "#     x=len(train[\"the extracted justification\"][index].split())\n",
    "#     if x > 150:\n",
    "#         train.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10155, 16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Preprocessing of the text, took care of basic pre-processing task only.\n",
    "\n",
    "def processSent(sent):\n",
    "    # To lowercase\n",
    "    sent = sent.lower()\n",
    "    #remove @username\n",
    "    sent = re.sub(r'@(\\w+)','',sent)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    sent = re.sub(r'#(\\w+)', '', sent)\n",
    "    \n",
    "    # Remove Punctuation and split 's, 't, 've with a space for filter    \n",
    "    sent = re.sub(r\"[-()\\\"«/;»:<>{}`+=~|.!&?',]\", \"\", sent)\n",
    "    \n",
    "    # remove the numerical values\n",
    "    sent = re.sub(r\"(\\s\\d+)\",\"\",sent) \n",
    "    \n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    sent = re.sub(r'\\&\\w*;', '', sent)\n",
    "    \n",
    "    # Remove tickers\n",
    "    sent = re.sub(r'\\$\\w*', '', sent)\n",
    "    \n",
    "    # Remove hyperlinks\n",
    "    sent = re.sub(r'https?:\\/\\/.*\\/\\w*', '', sent)\n",
    "   \n",
    "    # Remove whitespace (including new line characters)\n",
    "    sent = re.sub(r'\\s\\s+', ' ', sent)\n",
    "   \n",
    "    # Remove single space remaining at the front of the sent.\n",
    "    sent = sent.lstrip(' ') \n",
    "    \n",
    "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
    "    sent = ''.join(c for c in sent if c <= '\\uFFFF') \n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    surovell said the decline of coal started when...\n",
      "1    obama said he would have voted against the ame...\n",
      "2    the release may have a point that mikulskis co...\n",
      "3    crist said that the economic turnaround starte...\n",
      "4    but vos specifically used the word fired which...\n",
      "Name: the extracted justification, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = train.copy()\n",
    "data['the extracted justification'] = data['the extracted justification'].apply(processSent)\n",
    "data['stattement'] = data['statement'].apply(processSent)\n",
    "print(data[\"the extracted justification\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "half-true      2095\n",
       "false          1975\n",
       "mostly-true    1954\n",
       "true           1664\n",
       "barely-true    1637\n",
       "pants-fire      830\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs1=data[\"the extracted justification\"]\n",
    "docs2=data[\"statement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare tokenizer for \"the extracted justification\"\n",
    "MAX_NB_WORDS = 50000\n",
    "t1 = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "t1.fit_on_texts(docs1)\n",
    "vocab_size = len(t1.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs1 = t1.texts_to_sequences(docs1)\n",
    "print(encoded_docs1)\n",
    "# pad documents to a max length of 100 words\n",
    "max_length = 100\n",
    "padded_docs1 = pad_sequences(encoded_docs1, maxlen=max_length, padding='post')\n",
    "print(padded_docs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare tokenizer \"statement\"\n",
    "MAX_NB_WORDS = 50000\n",
    "t2 = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "t2.fit_on_texts(docs2)\n",
    "vocab_size = len(t2.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs2 = t2.texts_to_sequences(docs2)\n",
    "# print(encoded_docs2)\n",
    "# pad documents to a max length of 50 words\n",
    "max_length = 50\n",
    "padded_docs2 = pad_sequences(encoded_docs2, maxlen=max_length, padding='post')\n",
    "# print(padded_docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10155, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10155, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (10155, 6)\n"
     ]
    }
   ],
   "source": [
    "Y_train = pd.get_dummies(train['label']).values\n",
    "print('Shape of label tensor:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachin\\Desktop\\practice\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\sachin\\Desktop\\practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "import numpy as np\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,encoding=\"utf8\")\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 1917495  words loaded!\n"
     ]
    }
   ],
   "source": [
    "embedding_glove =loadGloveModel(\"glove.42B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachin\\Desktop\\fake news\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\sachin\\Desktop\\fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1917495"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embedding_glove['men']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs1 \"statement\"\n",
    "embedding_matrix1 = zeros((MAX_NB_WORDS, 300))\n",
    "for word, i in t1.word_index.items():\n",
    "\tembedding_vector = embedding_glove.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix1[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs2 \" the extracted justification\"\n",
    "embedding_matrix2 = zeros((MAX_NB_WORDS, 300))\n",
    "for word, i in t2.word_index.items():\n",
    "\tembedding_vector = embedding_glove.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix2[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [-0.20838  , -0.14932  , -0.017528 , ..., -0.54066  ,  0.21199  ,\n",
       "        -0.0094357],\n",
       "       [-0.24837  , -0.45461  ,  0.039227 , ...,  0.053097 ,  0.15458  ,\n",
       "        -0.38053  ],\n",
       "       ...,\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [-0.20838  , -0.14932  , -0.017528 , ..., -0.54066  ,  0.21199  ,\n",
       "        -0.0094357],\n",
       "       [ 0.068507 , -0.023344 ,  0.28271  , ..., -0.23447  ,  0.4055   ,\n",
       "        -0.28067  ],\n",
       "       ...,\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from LSTM import lstm_word_embeddin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing from layers.py file\n",
    "from layers import AttentionWithContext,Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model for LSTM with variable number of layers, attention , bi_directional,pre_trained word_embedding.\n",
    "def lstm_word_embedding(MAX_NB_WORDS, hidden_units, num_layers, max_sequence_length, is_attention, is_bidirectional, word_embedding):\n",
    "\ttimesteps = max_sequence_length\n",
    "\tnum_classes = 6\n",
    "\tembedding_dim = word_embedding.shape[1]\n",
    "    \n",
    "\tadam = optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(50000, 300, input_length=timesteps, weights=[word_embedding],trainable=False))\n",
    "        #model.add(Embedding(MAX_NB_WORDS, embedding_dim, input_length=50, weights=[word_embedding],trainable=False))\n",
    "\t#model.add(Embedding(50000, 300, input_length=50, weights=[word_embedding],trainable=False))\n",
    "       \n",
    "\tfor i in range(num_layers):\n",
    "\t\treturn_sequences = is_attention or (num_layers > 1 and i < num_layers-1)\n",
    "\n",
    "\t\tif is_bidirectional:\n",
    "\t\t\tmodel.add(Bidirectional(LSTM(hidden_units, return_sequences=return_sequences, dropout=0.2, kernel_initializer=initializers.glorot_normal(seed=777), bias_initializer='zeros')))\n",
    "\t\telse:\n",
    "\t\t\tmodel.add(LSTM(hidden_units, return_sequences=return_sequences, dropout=0.2, kernel_initializer=initializers.glorot_normal(seed=777), bias_initializer='zeros'))\n",
    "\t\t\n",
    "\t\tif is_attention and i==num_layers-1:\n",
    "\t\t\tmodel.add(AttentionWithContext())\n",
    "\t\t\tmodel.add(Addition())\n",
    "\n",
    "\t#model.add(Dense(num_classes, activation='softmax', kernel_initializer=initializers.glorot_normal(seed=777), bias_initializer='zeros'))\n",
    "\t#model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=[\"accuracy\"])\n",
    "\t#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#model for \"the extracted justification\"\n",
    "model1=lstm_word_embedding(MAX_NB_WORDS,320,2,max_sequence_length=100,is_attention=True,is_bidirectional=False,word_embedding=embedding_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-93bfed006681>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglorot_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m777\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'zeros'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madam\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "model1.add(Dense(6, activation='softmax', kernel_initializer=initializers.glorot_normal(seed=777), bias_initializer='zeros'))\n",
    "model1.compile(loss='categorical_crossentropy',optimizer=adam,metrics=[\"accuracy\"])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model for \"statement\"\n",
    "model2=lstm_word_embedding(MAX_NB_WORDS,320,2,max_sequence_length=50,is_attention=True,is_bidirectional=False,word_embedding=embedding_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 9139 samples, validate on 1016 samples\n",
      "Epoch 1/5\n",
      "9139/9139 [==============================] - 105s 11ms/step - loss: 1.7650 - acc: 0.2031 - val_loss: 1.7662 - val_acc: 0.1919\n",
      "Epoch 2/5\n",
      "9139/9139 [==============================] - 89s 10ms/step - loss: 1.7543 - acc: 0.2123 - val_loss: 1.7651 - val_acc: 0.1929\n",
      "Epoch 3/5\n",
      "9139/9139 [==============================] - 91s 10ms/step - loss: 1.7509 - acc: 0.2088 - val_loss: 1.7644 - val_acc: 0.2156\n",
      "Epoch 4/5\n",
      "9139/9139 [==============================] - 97s 11ms/step - loss: 1.7479 - acc: 0.2149 - val_loss: 1.7642 - val_acc: 0.2018\n",
      "Epoch 5/5\n",
      "9139/9139 [==============================] - 91s 10ms/step - loss: 1.7458 - acc: 0.2221 - val_loss: 1.7639 - val_acc: 0.2156\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "batch_size = 64\n",
    "history = model.fit(padded_docs1, Y_train,validation_split=0.1,epochs=epochs,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 25.782779\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, Y_train, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate\n",
    "from keras.models import Model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, bias_initializer=\"zeros\", return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, bias_initializer=\"zeros\", return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#model 1 for \"the extracted justification\"\n",
    "model1_in = keras.layers.Input(shape=(100,))\n",
    "# model1=Sequential()\n",
    "model1_out = Embedding(MAX_NB_WORDS, 300, weights=[embedding_matrix1], input_length=100, trainable=False)(model1_in)\n",
    "\n",
    "model1_out=LSTM(128,dropout_U=0.2,dropout_W=0.2,bias_initializer='zeros',return_sequences=True)(model1_out)\n",
    "model1_out=LSTM(128,dropout_U=0.2,dropout_W=0.2,bias_initializer='zeros',return_sequences=True)(model1_out)\n",
    "#model.add(Activation('softmax'))\n",
    "model1_out=AttentionWithContext()(model1_out)\n",
    "model1_out=Addition()(model1_out)\n",
    "model1_out = keras.layers.Dense(6, activation='relu')(model1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, bias_initializer=\"zeros\", return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \"\"\"\n",
      "C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, bias_initializer=\"zeros\", return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#model for \"statement\"\n",
    "model2_in = keras.layers.Input(shape=(50,))\n",
    "# model1=Sequential()\n",
    "model2_out = Embedding(MAX_NB_WORDS, 300, weights=[embedding_matrix2], input_length=50, trainable=False)(model2_in)\n",
    "\n",
    "model2_out=LSTM(128,dropout_U=0.2,dropout_W=0.2,bias_initializer='zeros',return_sequences=True)(model2_out)\n",
    "model2_out=LSTM(128,dropout_U=0.2,dropout_W=0.2,bias_initializer='zeros',return_sequences=True)(model2_out)\n",
    "#model.add(Activation('softmax'))\n",
    "model2_out=AttentionWithContext()(model2_out)\n",
    "model2_out=Addition()(model2_out)\n",
    "model2_out = keras.layers.Dense(6, activation='relu')(model2_out)\n",
    "# model2_out.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged=Concatenate()([model1_out,model2_out])\n",
    "# merged\n",
    "# concatenated = concatenate([model1_out, model2_out])\n",
    "out = Dense(6, activation='softmax', name='output_layer', kernel_initializer=initializers.glorot_normal(seed=777), bias_initializer='zeros')(merged)\n",
    "\n",
    "merged_model = Model([model1_in, model2_in], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 300)     15000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 50, 300)      15000000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 100, 128)     219648      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 50, 128)      219648      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 100, 128)     131584      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 50, 128)      131584      lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_1 (Atten (None, 100, 128)     16640       lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_2 (Atten (None, 50, 128)      16640       lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "addition_1 (Addition)           (None, 128)          0           attention_with_context_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "addition_2 (Addition)           (None, 128)          0           attention_with_context_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            774         addition_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            774         addition_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 12)           0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 6)            78          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 30,737,370\n",
      "Trainable params: 737,370\n",
      "Non-trainable params: 30,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "merged_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sachin\\Anaconda3\\envs\\jupyter\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 9139 samples, validate on 1016 samples\n",
      "Epoch 1/5\n",
      "9139/9139 [==============================] - 108s 12ms/step - loss: 1.7573 - acc: 0.2014 - val_loss: 1.7478 - val_acc: 0.2057\n",
      "Epoch 2/5\n",
      "9139/9139 [==============================] - 90s 10ms/step - loss: 1.7429 - acc: 0.2063 - val_loss: 1.7418 - val_acc: 0.2037\n",
      "Epoch 3/5\n",
      "9139/9139 [==============================] - 99s 11ms/step - loss: 1.7388 - acc: 0.2069 - val_loss: 1.7392 - val_acc: 0.2067\n",
      "Epoch 4/5\n",
      "9139/9139 [==============================] - 112s 12ms/step - loss: 1.7360 - acc: 0.2069 - val_loss: 1.7361 - val_acc: 0.2096\n",
      "Epoch 5/5\n",
      "9139/9139 [==============================] - 109s 12ms/step - loss: 1.7338 - acc: 0.2110 - val_loss: 1.7347 - val_acc: 0.2116\n"
     ]
    }
   ],
   "source": [
    "adam = optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "# model.add(Dense(6, activation='softmax', kernel_initializer=initializers.glorot_normal(seed=777), bias_initializer='zeros'))\n",
    "merged_model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=[\"accuracy\"])\n",
    "epochs=5\n",
    "batch_size = 64\n",
    "history = merged_model.fit([padded_docs1,padded_docs2], Y_train,validation_split=0.1,epochs=epochs,batch_size = batch_size)\n",
    "# merged_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
